Advanced Discrete Simulation
Assignment 1
Group 14
20 February 2019

**Overview of Data Cleaning Files:
Road_cleaning_FINAL.py - script that cleans the road data
BurningMoreBridges.R - script that cleans the bride data, based on the cleaned road data

**Road Cleaning
First, we clean the road data from .lrps.htm files (original from the ministry) with a python script. Inside the script, the following functions are defined and called:
    1. The input files are "RoadName".lrps.htm files and a list of all the road names.
    2. import_data() this function reads in the road files ("name".lrps.htm) based on a list of names of all the roads in Bangladesh. It then iterates through all the road files, opening and closing each one in turn, while reading in the data to data frames and performing some first-pass data cleaning (removing columns of NAs, renaming column headers, etc.). It then calls determine_outliers on the data, and afterwards saves the cleaned data to a tab separated file, with columns that contain data about LRP Number, Road Change, LRP Type, Description, Latitude, Longitude, and Road.
    3. determine_outliers() calculates (for the longitude and latitude separately and simultaneously) the difference between a longitude or latitude entry, and the corresponding value in the entry above and below it. It then takes the standard deviation of these difference values and uses that to create a cutoff value. When a difference is above a cutoff value, the function find_replace_outliers() is called, otherwise the data is appended to a data frame that stores the cleaned data.
    4. find_replace_outliers() collects the outliers in the road data (identified in determine_outliers) into a data frame of outliers. It then identifies outliers that are unreasonably far away from both the entry above it and the entry below it (i.e. isolating the outlier (in the case of a single outlier) from the points above and below it that are not outliers themselves). These outliers are replaced by an interpolation (simple mean) of the entries above and below the outlier entry. The interpolated points are then merged with the data frame and used to replace the outlier points they correspond to.
    5. The output after calling these three function is cleaned_all_road_data.csv

**Bridge Cleaning
Second, we clean the bridge data from the BMMS_overview.csv
    1. The input files are the .csv output from the road cleaning, the BMMS_overview.csv, and a list of all the road names.
    2. Then the script iterates through each bridge point and compares it to the lrp in the road data to see if anything related to it has changed.
    3. If so, the corresponding bridge point needs regenerating.
    4. There are 4 ways of converting bridge point location from the road data.
	a. If the LRP number from the road data and the bridge data match, but their latitude and longitude coordinates do not, the latitude and longitude coordinates for the bridge data are replaced by the latitude and longitude coordinates from the matching (cleaned) road data.
	b. If the short line distance between a bridge longitude and latitude and a road point longitude and latitude is less than a threshold of 20 meters, then the latitude and longitude coordinates for the bridge data are replaced by the latitude and longitude coordinates from the road data with the closest coordinate match.
	c. If the chainage difference (difference between road and bridge chainage) is less than the threshold of 10 meters, than the latitude and longitude coordinates for the bridge data are replaced by the latitude and longitude coordinates from the road data with the closest chainage match.
	d. If the chainage difference (difference between road and bridge chainage) is less than the threshold of 200 meters, (##update description of this)
    5. If nothing else matched closely enough from the four previous methods, then the bridge location data is replaced by an interpolation (simple mean) of the entries above and below the outlier entry.
    6. The output files are a version of the BMMS_overview with corrected data (BMMS_Overview_Cleaned.csv) and a change log (change_log.csv)

**Preparation for Java (Save as TCV)
Third, another R script (Sava_as_TCV.R) generates the .tcv of the roads, that is required for simulation
    1. The BMMS_Overview.csv (##update description of this / ##needs to be updated to BMMS_Overview_Cleaned.csv in R file?) is adapted to the required format form the .csv file generated during the data cleaning.
    2. The description of the TCV file from brightspace (below) is used to reformat the cleaned bridge data in the proper format for import. 
          "infrastructure/_roads.tcv: a tab-separated text-file with processed information from the RMMS dataset. It contains a first line with explanation, and the following tab-separated data starting from line 2, with one road per line.
              o road name: the official name of the road
              o lrp1: the name of the first LRP of the road
              o lat1: the latitude of lrp1
              o lon1: the longitude of lrp1
              o ...
              o lrpn: the name of the last LRP of the road
              o latn: the latitude of lrpn
              o lonn: the longitude of lrpn"
    3. The output is saved as data.tcv

**Types of Inaccuracy
##Notes from class -- to do this afternoon
Maybe make a summary table like this:
  what we cleaned			what type of data issue it was (syntactic, semantic, pragmatic etc etc)			how we cleaned it

  
**Dealing with semantic inaccuracy in Chainage 
 There are 29 bridges whose Lat/Lon are estimated with 'road_interpolate' and are longer than 50m and on an N road.
 There 2 bridges whose Lat/Lon are estimated with 'road_chainage' and are longer than 50m and on an N road. 
 For a number of reasons, we have decided that the error checking of these bridges should be completed manually 
 i.e. look up the bridge name on google maps and check the latitude and longitude co-ordinates.
 Justification:
    1. Difficulty in fixing: there are reasonably quick ways of automating the estimate of the chainage of LRP point in a road (some % of the straight-line distance between points).
    But there is no reliable way of checking that the chainage of the bridge points are correct other than manually. 
    2. Errors have low impact on pragmatic precision because these bridges are still on the correct road. 
    The fact that they may be a certain number of kilometers (max the difference in chainage changes due to maintenance/straightening is order of magnitude tens of km)
    will not affect the analysis. We assume that the assessment of the importance of a bridge is mostly due to the areas it links, and the road it is on, which is preserved.
    
Error searching revealed the following errors/changes:



**Semantic incompleteness error in the bridge data:
There are entire roads in the RMMS zip (##update exact number here please) for which no corresponding files of bridges exist in the BMMS zip.
This means that for entire roads, no bridges are included. 
We have included this list in our output files and have implemented a limited repair due to time constraints but would like the client to be made aware of this incompleteness.
The concept of the remedy is to identify roads named in RMMMS zip, and identify roads named in BMMS zip and compare these lists.
The script missing_roads.py takes the list of roads that do not have any bridge data recorded in BMMS (generated in bridge_cleaning.r)
and sorts for the lrp points on these (X number of) roads by type (Bridge/Culvert). These lrp points are available for both start/end of bridges (sometime both but not always)
so we only took the lrp points that indicated the start of a bridge. We estimated the length of these bridges by a default values, that are taken from the average length of bridges on the category:
N = 25m
R = 17m
Z = 14m
These averages were quickly calculated in excel.
The missing_roads.py script generates an output of these 'missing bridges' to be appended to the BMMS_Overview.csv.
Limitations:
	1. Certainly this method is rough in semantic accuracy (lenghts/start points are estimates and unverified)
	2. Secondly, semantic incompleteness is introduced for each of these 'filled in' bridges as there are many fields in BMMS_Overview.csv for which we do not have information to fill in.
	3. This inherently generates a degree of type insufficiency in those particular bridge data points as they do not contain detailed information of type structure and grade quality of bridges.
	This is particularly important for the Worldbank investment analysis and certainly a limitation of this method.
	
However, the alternative is to keep a known large number of semantic incompleteness in the bridge dataset, and also that there would be entrie roads for which no bridges are included at all.
The impact of this on pragmatic incompleteness would be unacceptable for the worldbank study so some solution must be applied for this error type, in our opinion, despite its limitations.
In a more comprehensive data preparation project, it would be advisable to return to the Ministry of Transport with the finite list of roads for which no folder exists in the BMMS zip, and request these.

**On the note of pragmatism, it is important that there is accuracy in the grade quality of each bridge for this study. Unfortunately, we have completed no correction/checking of this field
in our data cleaning we could think of no reasonable way to cross-check this field with the data provided to us. It would be recommended to manually check (ie. google the name) a sample population (n>30) of the entire bridge set,
to be able to give an estimate of the sematic accuracy of the Quality of bridges. At least if we cannot feasibly fix all these errors,
it would be useful when interpreting the results to know how reliable this field is, if the WorldBank is considering its investments.

**The possiblity of duplicated bridge data was investigated using the following chunk of code:

		> length( unique(bridge_data$structureNr))
		[1] 21407
		> length(bridge_data$structureNr)
		[1] 21407
No duplicates found :0
The limitation of this method is that it is assumed that structureNr is semanticly accurate and consistent and unique. 
This is a reasonable assumption as we have completed a short visual scan of a selection of bridges and it seems to be unique.



**In terms of Presentation Suitability, we found no error with the display of the data in the Java application with the roads, bridges, waterways plotted.
This visual display is useful to set context for the Worldbank's problem and to visually inspect for errors. The colour-coding of bridges by quality was especially suitable.
One draw back of this visualisation is that it tends to emphasize the prominence of small errors to the viewer.
One single errored co-ordinate produces a spike in the road that is much more visually obstructve that 1000s of correct co-ordinate points.

